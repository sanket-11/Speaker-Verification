{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import librosa\n",
    "import numpy as np\n",
    "from itertools import combinations, product\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 16180)\n",
      "(200, 22631)\n"
     ]
    }
   ],
   "source": [
    "with open('hw4_trs.pkl', 'rb') as f:\n",
    "    trs_data = pickle.load(f)\n",
    "print(trs_data.shape)\n",
    "with open('hw4_tes.pkl', 'rb') as f:\n",
    "    tes_data = pickle.load(f)\n",
    "print(tes_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process STFT and pad training data to match with test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs_stft = []\n",
    "tes_stft = []\n",
    "\n",
    "for i in range(500):\n",
    "    X=librosa.stft(trs_data[i], n_fft=1024, hop_length=512)\n",
    "    X=np.pad(X,((0,0),(0,45-X.shape[1])),'constant')\n",
    "    trs_stft.append(np.abs(np.transpose(X)))\n",
    "for i in range(200):\n",
    "    T=librosa.stft(tes_data[i], n_fft=1024, hop_length=512)\n",
    "    tes_stft.append(np.abs(np.transpose(T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute True pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_pairs(data):\n",
    "    final_pairs = []\n",
    "    random.seed(599)\n",
    "    for i in range(0,len(data),10):\n",
    "        comb = list(combinations(data[i:i+10], 2))\n",
    "        final_pairs.append(random.choices(comb, k = num_pairs))\n",
    "    return final_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs_true_pairs = get_true_pairs(trs_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_true_pairs = get_true_pairs(tes_stft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute False pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false_pairs(data):\n",
    "    final_false = []\n",
    "    for i in range(0,len(data),10):\n",
    "        random.seed(599)\n",
    "        current = list(range(i,i+10))\n",
    "        first = list(range(0,i))\n",
    "        last = list(range(i+10,len(data)))\n",
    "        final = first+ last\n",
    "        prod = list(product(current, final))\n",
    "        false_comb = (random.choices(prod, k = num_pairs))\n",
    "        comb_list= []\n",
    "        for comb in false_comb:\n",
    "            comb_list.append((data[comb[0]],data[comb[1]]))\n",
    "        final_false.append(comb_list)\n",
    "        \n",
    "    return final_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs_false_pairs = get_false_pairs(trs_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_false_pairs = get_false_pairs(tes_stft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(true_pairs, false_pairs):\n",
    "    final_true_pairs = []\n",
    "    final_false_pairs = []\n",
    "    true_labels = []\n",
    "    false_labels = []\n",
    "    for i in true_pairs:\n",
    "        for j in i:\n",
    "            final_true_pairs.append(j)\n",
    "            true_labels.append(1)\n",
    "    \n",
    "    for i in false_pairs:\n",
    "        for j in i:\n",
    "            final_false_pairs.append(j)\n",
    "            false_labels.append(0)\n",
    "    \n",
    "    assert len(final_true_pairs) == len(final_false_pairs) == len(true_labels) == len(false_labels)\n",
    "    \n",
    "    temp_data = []\n",
    "    temp_labels = []\n",
    "    for i in range(0, len(final_true_pairs), num_pairs):\n",
    "        temp_data.append(final_true_pairs[i:i+num_pairs])\n",
    "        temp_data.append(final_false_pairs[i:i+num_pairs])\n",
    "        temp_labels.append(true_labels[i:i+num_pairs])\n",
    "        temp_labels.append(false_labels[i:i+num_pairs])\n",
    "    \n",
    "    final_labels =[]\n",
    "    final_data = []\n",
    "    for i in temp_data:\n",
    "        for j in i:\n",
    "            final_data.append(j)\n",
    "    for i in temp_labels:\n",
    "        for j in i:\n",
    "            final_labels.append(j)\n",
    "    \n",
    "    return final_data, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels=  generate_data(trs_true_pairs, trs_false_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = generate_data(tes_true_pairs, tes_false_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 2, 45, 513)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network architecture\n",
    "\n",
    "For the architecture, I chose to go with 6 convolution layers followed by max pooling and dropout after each 2 convolution layers and followed by a dense layers at the end.\n",
    "\n",
    "For the loss function, I went with constrastive loss. Adam Optimizer was chosen with learning rate 0.0001 and trained the network for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_network(inputs, re = False):\n",
    "    with tf.variable_scope('basenet',reuse=re):\n",
    "        h1 = tf.layers.conv2d(inputs, filters = 32, kernel_size=(3,7), strides=(2,2), activation=\"relu\", padding = \"same\", kernel_initializer = tf.glorot_uniform_initializer)\n",
    "        print(\"h1\",h1.shape)\n",
    "\n",
    "        h2 = tf.layers.conv2d(h1, filters = 32, kernel_size=(3,7), strides=(1,1), activation=\"relu\", padding = \"same\", kernel_initializer = tf.glorot_uniform_initializer)\n",
    "        print(\"h2\",h2.shape)\n",
    "\n",
    "        h2_maxpool = tf.layers.max_pooling2d(h2, pool_size=(2,2), strides =(2,2) , padding = 'valid')\n",
    "        print(\"h2_maxpool\", h2_maxpool.shape)\n",
    "        dropout1=tf.layers.dropout(h2_maxpool,0.2)\n",
    "\n",
    "        h3 = tf.layers.conv2d(dropout1, filters = 64, kernel_size=(3,7), strides=(2,2), activation=\"relu\", padding = \"same\",kernel_initializer = tf.glorot_uniform_initializer)\n",
    "        print(\"h3\",h3.shape)\n",
    "\n",
    "        h4 = tf.layers.conv2d(h3, filters = 64, kernel_size=(3,7), strides=(1,1), activation=\"relu\", padding = \"same\", kernel_initializer = tf.glorot_uniform_initializer)\n",
    "        print(\"h4\",h4.shape)\n",
    "\n",
    "        h4_maxpool = tf.layers.max_pooling2d(h4, pool_size=(2,2), strides =(2,2), padding = 'valid')\n",
    "        print(\"h4_maxpool\", h4_maxpool.shape)\n",
    "        dropout2=tf.layers.dropout(h4_maxpool,0.2)\n",
    "        \n",
    "        h5 = tf.layers.conv2d(dropout2, filters = 128, kernel_size=(3,5), strides=(1,2), activation=\"relu\", padding = \"same\", kernel_initializer = tf.glorot_uniform_initializer)\n",
    "        print(\"h5\",h5.shape)\n",
    "        h6 = tf.layers.conv2d(h5, filters = 128, kernel_size=(3,5), strides=(1,1), activation=\"relu\", padding = \"same\", kernel_initializer = tf.glorot_uniform_initializer)\n",
    "        print(\"h6\",h6.shape)\n",
    "        h6_maxpool = tf.layers.max_pooling2d(h6, pool_size=(2,2), strides =(2,2), padding = 'valid')\n",
    "        print(\"h6_maxpool\", h6_maxpool.shape)\n",
    "        dropout3=tf.layers.dropout(h6_maxpool,0.2)\n",
    "\n",
    "        flatten_layer = tf.layers.flatten(dropout3)\n",
    "        print(\"flatten\", flatten_layer.shape)\n",
    "        \n",
    "        dense_1 = tf.layers.dense(flatten_layer, 512, activation = \"relu\", kernel_initializer = tf.glorot_uniform_initializer)\n",
    "        print(\"dense1\" ,dense_1.shape)\n",
    "        return dense_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Contrastive Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ardiya/siamesenetwork-tensorflow/blob/master/model.py\n",
    "def contrastive_loss(a, b, label, margin = 1):\n",
    "    with tf.variable_scope('contrastive',reuse=False):\n",
    "        distance = tf.sqrt(tf.maximum(tf.reduce_sum(tf.pow(a - b, 2), 1), 1e-6))\n",
    "        print(\"distance\", distance.shape)\n",
    "        similarity = label * tf.square(distance)                                           # keep the similar label (1) close to each other\n",
    "        dissimilarity = (1 - label) * tf.square(tf.maximum((margin - distance), 0))        # give penalty to dissimilar label if the distance is bigger than margin\n",
    "        return tf.reduce_mean(dissimilarity + similarity) , distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input1 (?, 45, 513, 1)\n",
      "input2 (?, 45, 513, 1)\n",
      "y (?, 1)\n",
      "h1 (?, 23, 257, 32)\n",
      "h2 (?, 23, 257, 32)\n",
      "h2_maxpool (?, 11, 128, 32)\n",
      "h3 (?, 6, 64, 64)\n",
      "h4 (?, 6, 64, 64)\n",
      "h4_maxpool (?, 3, 32, 64)\n",
      "h5 (?, 3, 16, 128)\n",
      "h6 (?, 3, 16, 128)\n",
      "h6_maxpool (?, 1, 8, 128)\n",
      "flatten (?, 1024)\n",
      "dense1 (?, 512)\n",
      "dense2 (?, 256)\n",
      "h1 (?, 23, 257, 32)\n",
      "h2 (?, 23, 257, 32)\n",
      "h2_maxpool (?, 11, 128, 32)\n",
      "h3 (?, 6, 64, 64)\n",
      "h4 (?, 6, 64, 64)\n",
      "h4_maxpool (?, 3, 32, 64)\n",
      "h5 (?, 3, 16, 128)\n",
      "h6 (?, 3, 16, 128)\n",
      "h6_maxpool (?, 1, 8, 128)\n",
      "flatten (?, 1024)\n",
      "dense1 (?, 512)\n",
      "dense2 (?, 256)\n",
      "distance (?,)\n",
      "100 45\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#two inputs for part of each pair\n",
    "input1 = tf.placeholder(tf.float32, [None, 45, 513, 1], name='input1')\n",
    "print(\"input1\",input1.shape)\n",
    "\n",
    "input2 = tf.placeholder(tf.float32, [None, 45, 513, 1], name='input2')\n",
    "print(\"input2\",input2.shape)\n",
    "\n",
    "#output labels\n",
    "y= tf.placeholder(tf.float32, [None,1], name=\"pred\")\n",
    "print(\"y\", y.shape)\n",
    "\n",
    "#pass the inputs to basenetwork\n",
    "output1 = base_network(input1, re = False)\n",
    "output2 = base_network(input2, re = True)  \n",
    "\n",
    "processed_output1=tf.layers.batch_normalization(output1,beta_regularizer=\"l2\")\n",
    "processed_output2=tf.layers.batch_normalization(output2,beta_regularizer=\"l2\")\n",
    "loss, eudistance = contrastive_loss(processed_output1, processed_output2, y)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "epochs = 30\n",
    "train_batches = int(len(train_data)/(num_pairs))\n",
    "train_batch_size = int(num_pairs)\n",
    "print(train_batches, train_batch_size)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writer = tf.summary.FileWriter('./a4graph', graph = tf.get_default_graph())\n",
    "# writer.close()\n",
    "len(train_data)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.23409556843340396\n",
      "Epoch: 1 Loss: 0.18985908813774585\n",
      "Epoch: 2 Loss: 0.16414411820471286\n",
      "Epoch: 3 Loss: 0.14394067943096162\n",
      "Epoch: 4 Loss: 0.1267944796010852\n",
      "Epoch: 5 Loss: 0.11109062030911446\n",
      "Epoch: 6 Loss: 0.0982693406380713\n",
      "Epoch: 7 Loss: 0.08918127892538905\n",
      "Epoch: 8 Loss: 0.0823256360553205\n",
      "Epoch: 9 Loss: 0.07628320232033729\n",
      "Epoch: 10 Loss: 0.07329390626400709\n",
      "Epoch: 11 Loss: 0.07070019045844673\n",
      "Epoch: 12 Loss: 0.06683644415810704\n",
      "Epoch: 13 Loss: 0.06579456059262156\n",
      "Epoch: 14 Loss: 0.0647602802887559\n",
      "Epoch: 15 Loss: 0.06513416316360235\n",
      "Epoch: 16 Loss: 0.05715098180808127\n",
      "Epoch: 17 Loss: 0.05228254922665656\n",
      "Epoch: 18 Loss: 0.046440651756711304\n",
      "Epoch: 19 Loss: 0.04479751555249095\n",
      "Epoch: 20 Loss: 0.04265551464166492\n",
      "Epoch: 21 Loss: 0.04335604273248464\n",
      "Epoch: 22 Loss: 0.04152660571038723\n",
      "Epoch: 23 Loss: 0.03820983230834827\n",
      "Epoch: 24 Loss: 0.03587978850584477\n",
      "Epoch: 25 Loss: 0.036245230636559427\n",
      "Epoch: 26 Loss: 0.036863068342208864\n",
      "Epoch: 27 Loss: 0.03440144557971507\n",
      "Epoch: 28 Loss: 0.03297313915332779\n",
      "Epoch: 29 Loss: 0.030908904070965946\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess_a4_part1 = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess_a4_part1.run(init)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    temp_loss = 0\n",
    "    #print(epoch)\n",
    "    for i in range(0,len(train_data), train_batch_size):\n",
    "     #   print(i, i+train_batch_size)\n",
    "        loss_train,_ = sess_a4_part1.run([loss, optimizer], feed_dict = {input1: np.expand_dims(np.array(train_data)[i:i+train_batch_size,0],-1),\\\n",
    "                                                                    input2: np.expand_dims(np.array(train_data)[i:i+train_batch_size,1],-1),\\\n",
    "                                                                 y: np.expand_dims(np.array(train_labels)[i:i+train_batch_size],1)})\n",
    "        temp_loss+=loss_train\n",
    "    print(\"Epoch:\",epoch,\"Loss:\",temp_loss/train_batches)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#deine test batch size and batches\n",
    "test_batches = int(len(test_data)/(num_pairs))\n",
    "test_batch_size =int(num_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions on test data based on euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 45\n",
      "45 90\n",
      "90 135\n",
      "135 180\n",
      "180 225\n",
      "225 270\n",
      "270 315\n",
      "315 360\n",
      "360 405\n",
      "405 450\n",
      "450 495\n",
      "495 540\n",
      "540 585\n",
      "585 630\n",
      "630 675\n",
      "675 720\n",
      "720 765\n",
      "765 810\n",
      "810 855\n",
      "855 900\n",
      "900 945\n",
      "945 990\n",
      "990 1035\n",
      "1035 1080\n",
      "1080 1125\n",
      "1125 1170\n",
      "1170 1215\n",
      "1215 1260\n",
      "1260 1305\n",
      "1305 1350\n",
      "1350 1395\n",
      "1395 1440\n",
      "1440 1485\n",
      "1485 1530\n",
      "1530 1575\n",
      "1575 1620\n",
      "1620 1665\n",
      "1665 1710\n",
      "1710 1755\n",
      "1755 1800\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(0,len(test_data),test_batch_size):\n",
    "    print(i, i+train_batch_size)\n",
    "    predictions.append(sess_a4_part1.run(eudistance, feed_dict = {input1: np.expand_dims(np.array(test_data)[i:i+test_batch_size,0],-1),\\\n",
    "                                                                    input2: np.expand_dims(np.array(test_data)[i:i+test_batch_size,1],-1),\\\n",
    "                                                                   y: np.expand_dims(np.array(test_labels)[i:i+test_batch_size],1)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = []\n",
    "for i in predictions:\n",
    "    for j in i:\n",
    "        final_predictions.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate accuracy, we check if the predicted value is less than 0.5. If the euclidean distance(prediction) is smaller than 0.5, which is picked as an optimal value between 0 and 1, our prediction is closer to the correct value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.77777777777777\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "alltrue=[]\n",
    "allfalse=[]\n",
    "for i in range(len(test_labels)):\n",
    "    temp = 0\n",
    "    \n",
    "    if final_predictions[i] < 0.5:\n",
    "        temp=1\n",
    "    if temp==test_labels[i]:\n",
    "        correct_predictions+=1\n",
    "accuracy = count/len(test_labels)*100\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
