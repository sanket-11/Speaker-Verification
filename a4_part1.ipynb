{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Verification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import librosa\n",
    "import numpy as np\n",
    "from itertools import combinations, product\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 16180)\n",
      "(200, 22631)\n"
     ]
    }
   ],
   "source": [
    "with open('hw4_trs.pkl', 'rb') as f:\n",
    "    trs_data = pickle.load(f)\n",
    "print(trs_data.shape)\n",
    "with open('hw4_tes.pkl', 'rb') as f:\n",
    "    tes_data = pickle.load(f)\n",
    "print(tes_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "trs_stft = []\n",
    "#test\n",
    "tes_stft = []\n",
    "\n",
    "for i in range(500):\n",
    "    X=librosa.stft(trs_data[i], n_fft=1024, hop_length=512)\n",
    "    X=np.pad(X,((0,0),(0,45-X.shape[1])),'constant')\n",
    "    trs_stft.append(np.abs(np.transpose(X)))\n",
    "for i in range(200):\n",
    "    T=librosa.stft(tes_data[i], n_fft=1024, hop_length=512)\n",
    "    tes_stft.append(np.abs(np.transpose(T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of pairs to consider for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer True pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_pairs(data):\n",
    "    final_pairs = []\n",
    "    for i in range(0,len(data),10):\n",
    "        comb = list(combinations(data[i:i+10], 2))\n",
    "        final_pairs.append(random.choices(comb, k = num_pairs))\n",
    "    return final_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs_true_pairs = get_true_pairs(trs_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_true_pairs = get_true_pairs(tes_stft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Computer False pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false_pairs(data):\n",
    "    final_false = []\n",
    "    for i in range(0,len(data),10):\n",
    "        current = list(range(i,i+10))\n",
    "        first = list(range(0,i))\n",
    "        last = list(range(i+10,len(data)))\n",
    "        final = first+ last\n",
    "        prod = list(product(current, final))\n",
    "        false_comb = (random.choices(prod, k = num_pairs))\n",
    "        comb_list= []\n",
    "        for comb in false_comb:\n",
    "            comb_list.append((data[comb[0]],data[comb[1]]))\n",
    "        final_false.append(comb_list)\n",
    "        \n",
    "    return final_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs_false_pairs = get_false_pairs(trs_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_false_pairs = get_false_pairs(tes_stft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(true_pairs, false_pairs):\n",
    "    final_true_pairs = []\n",
    "    final_false_pairs = []\n",
    "    true_labels = []\n",
    "    false_labels = []\n",
    "    for i in true_pairs:\n",
    "        for j in i:\n",
    "            final_true_pairs.append(j)\n",
    "            true_labels.append(1)\n",
    "\n",
    "    for i in false_pairs:\n",
    "        for j in i:\n",
    "            final_false_pairs.append(j)\n",
    "            false_labels.append(0)\n",
    "    final_labels = true_labels + false_labels\n",
    "    final_data = final_true_pairs + final_false_pairs\n",
    "    return final_data, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = generate_data(trs_true_pairs, trs_false_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = generate_data(tes_true_pairs, tes_false_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2, 45, 513)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Lambda, MaxPool2D, Conv2D, Flatten\n",
    "import keras.backend as K\n",
    "from keras.models import Model,Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import Input\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanky\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "output1 Tensor(\"sequential_1/dense_1/Relu:0\", shape=(?, 1024), dtype=float32)\n",
      "output2 Tensor(\"sequential_1_1/dense_1/Relu:0\", shape=(?, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "baseinput1 = Input(shape=(45,513,1))\n",
    "baseinput2 = Input(shape=(45,513,1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,9),activation='relu',strides=2, padding='same',input_shape=(45,513,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=1, padding='valid'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,5),activation='relu',strides=2, padding='same',input_shape=(45,513,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=1, padding='valid'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1,3),activation='relu',strides=2, padding='same',input_shape=(45,513,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=1, padding='valid'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1,1),activation='relu',strides=2, padding='same',input_shape=(45,513,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=1, padding='valid'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "baseoutput1= model(baseinput1)\n",
    "baseoutput2 = model(baseinput2)\n",
    "\n",
    "print(\"output1\",baseoutput1)\n",
    "print(\"output2\",baseoutput2)\n",
    "# distance=Lambda(lambda tensors:K.abs(tensors[0]-tensors[1]))([output1,output2])\n",
    "# print(distance.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 23, 257, 64)       1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 256, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 128, 64)       61504     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 127, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 64, 64)         12352     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 63, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 32, 64)         4160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 31, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2032640   \n",
      "=================================================================\n",
      "Total params: 2,112,448\n",
      "Trainable params: 2,112,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([baseoutput1, baseoutput2])\n",
    "\n",
    "Siamese_model = Model([baseinput1, baseinput2], distance)\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) +\n",
    "                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def acc(a, b):\n",
    "    return K.mean(K.equal(a, K.cast(b<0.5, a.dtype)))\n",
    "    \n",
    "Siamese_model.compile(loss=contrastive_loss,optimizer=optimizers.Adam(0.0001), metrics = [acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_layer=Dense(1,activation='sigmoid')#(distance)\n",
    "#Siamese_model=Model(inputs=[first,second],outputs=final_layer)\n",
    "# Siamese_model.compile(loss='binary_crossentropy',optimizer=optimizers.Adam(0.0001), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 45, 513, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 45, 513, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1024)         2112448     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,112,448\n",
      "Trainable params: 2,112,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanky\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\sanky\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 9s 4ms/step - loss: 0.3616 - acc: 0.5670 - val_loss: 0.2188 - val_acc: 0.6250\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1829 - acc: 0.7395 - val_loss: 0.1732 - val_acc: 0.7337\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1445 - acc: 0.8210 - val_loss: 0.1772 - val_acc: 0.7250\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1232 - acc: 0.8510 - val_loss: 0.1822 - val_acc: 0.7200\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1082 - acc: 0.8720 - val_loss: 0.1865 - val_acc: 0.7225\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0976 - acc: 0.8860 - val_loss: 0.1921 - val_acc: 0.7125\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0906 - acc: 0.9030 - val_loss: 0.2015 - val_acc: 0.7050\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0826 - acc: 0.9170 - val_loss: 0.1900 - val_acc: 0.7112\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0758 - acc: 0.9275 - val_loss: 0.2027 - val_acc: 0.7237\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0722 - acc: 0.9345 - val_loss: 0.1905 - val_acc: 0.7163\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0684 - acc: 0.9340 - val_loss: 0.2004 - val_acc: 0.7175\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0632 - acc: 0.9480 - val_loss: 0.2383 - val_acc: 0.7025\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0595 - acc: 0.9490 - val_loss: 0.2154 - val_acc: 0.7050\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0562 - acc: 0.9535 - val_loss: 0.2167 - val_acc: 0.6950\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0549 - acc: 0.9575 - val_loss: 0.2399 - val_acc: 0.6938\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0487 - acc: 0.9645 - val_loss: 0.2471 - val_acc: 0.6875\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0471 - acc: 0.9685 - val_loss: 0.2413 - val_acc: 0.7063\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0419 - acc: 0.9760 - val_loss: 0.2700 - val_acc: 0.6688\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0416 - acc: 0.9770 - val_loss: 0.2593 - val_acc: 0.6838\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0372 - acc: 0.9840 - val_loss: 0.2568 - val_acc: 0.6838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x219916d1dd8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Siamese_model.fit([np.expand_dims(np.array(train_data)[:,0],-1),np.expand_dims(np.array(train_data)[:,1],-1)],np.array(train_labels),\\\n",
    "                  batch_size=10,epochs=20,\\\n",
    "                  validation_data = ([np.expand_dims(np.array(test_data)[:,0],-1),np.expand_dims(np.array(test_data)[:,1],-1)],\\\n",
    "                 np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Siamese_model.predict([np.expand_dims(np.array(test_data)[:,0],-1),np.expand_dims(np.array(test_data)[:,1],-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40129486]\n",
      "[0.22594629]\n",
      "[0.3529852]\n",
      "[0.17945689]\n",
      "[0.23034655]\n",
      "[0.4260963]\n",
      "[0.39400625]\n",
      "[0.18588144]\n",
      "[0.25167125]\n",
      "[0.3243593]\n",
      "[0.41715217]\n",
      "[0.3349954]\n",
      "[0.5077506]\n",
      "[0.41287428]\n",
      "[0.3673569]\n",
      "[0.18099289]\n",
      "[0.22594629]\n",
      "[0.5454541]\n",
      "[0.5077506]\n",
      "[0.40129486]\n",
      "[0.20886666]\n",
      "[0.67007023]\n",
      "[0.19873625]\n",
      "[0.22744939]\n",
      "[0.35943824]\n",
      "[0.55953217]\n",
      "[0.29373392]\n",
      "[0.5921633]\n",
      "[0.32292774]\n",
      "[0.33193043]\n",
      "[0.43245068]\n",
      "[0.198351]\n",
      "[0.7841445]\n",
      "[0.61699903]\n",
      "[0.22744939]\n",
      "[0.73344487]\n",
      "[0.7841445]\n",
      "[0.7290576]\n",
      "[0.7290576]\n",
      "[0.1971048]\n",
      "[0.45370954]\n",
      "[0.45370954]\n",
      "[0.24057868]\n",
      "[0.56011426]\n",
      "[1.0946196]\n",
      "[1.0231754]\n",
      "[0.44992316]\n",
      "[0.4165552]\n",
      "[0.58732975]\n",
      "[0.86318344]\n",
      "[1.328255]\n",
      "[0.67197114]\n",
      "[0.7233881]\n",
      "[0.8191278]\n",
      "[0.33402947]\n",
      "[0.29884386]\n",
      "[0.20754388]\n",
      "[0.9160494]\n",
      "[0.5604217]\n",
      "[1.0231754]\n",
      "[0.39592913]\n",
      "[0.28678387]\n",
      "[0.3524798]\n",
      "[0.27409324]\n",
      "[0.29375604]\n",
      "[0.38393608]\n",
      "[0.22755039]\n",
      "[0.38393608]\n",
      "[0.41156653]\n",
      "[0.41156653]\n",
      "[0.39451241]\n",
      "[0.28004685]\n",
      "[0.24317922]\n",
      "[0.41156653]\n",
      "[0.2710624]\n",
      "[0.29375604]\n",
      "[0.2731707]\n",
      "[0.2710624]\n",
      "[0.35394278]\n",
      "[0.4524067]\n",
      "[0.9358004]\n",
      "[0.3688812]\n",
      "[0.5997052]\n",
      "[0.4070632]\n",
      "[0.7198109]\n",
      "[0.32862252]\n",
      "[0.4070632]\n",
      "[0.5335105]\n",
      "[0.7456485]\n",
      "[0.38550827]\n",
      "[0.8090965]\n",
      "[0.41584718]\n",
      "[0.5307797]\n",
      "[0.5997052]\n",
      "[0.761937]\n",
      "[0.48774198]\n",
      "[0.44124034]\n",
      "[0.861206]\n",
      "[0.38550827]\n",
      "[0.48326603]\n",
      "[0.2932433]\n",
      "[0.62706774]\n",
      "[0.6145188]\n",
      "[0.380865]\n",
      "[0.46803042]\n",
      "[0.5369548]\n",
      "[0.9173542]\n",
      "[0.28882]\n",
      "[0.42720553]\n",
      "[0.9173542]\n",
      "[0.28882]\n",
      "[0.36088955]\n",
      "[0.62313807]\n",
      "[0.44565377]\n",
      "[0.40444902]\n",
      "[0.9173542]\n",
      "[0.44993424]\n",
      "[0.59113175]\n",
      "[0.36088955]\n",
      "[0.62706774]\n",
      "[0.6094828]\n",
      "[0.94558364]\n",
      "[0.6094828]\n",
      "[0.6094828]\n",
      "[0.40458935]\n",
      "[0.27119896]\n",
      "[0.40133566]\n",
      "[0.4290372]\n",
      "[0.40133566]\n",
      "[0.6970265]\n",
      "[0.40133566]\n",
      "[0.4165395]\n",
      "[0.40458935]\n",
      "[0.24716482]\n",
      "[0.18049344]\n",
      "[0.26954007]\n",
      "[0.9238546]\n",
      "[1.0471239]\n",
      "[0.47641504]\n",
      "[0.40458935]\n",
      "[0.19887891]\n",
      "[1.0609751]\n",
      "[0.76449746]\n",
      "[0.6787894]\n",
      "[0.36061096]\n",
      "[0.9395917]\n",
      "[0.43760282]\n",
      "[0.16512342]\n",
      "[0.47989255]\n",
      "[0.2788555]\n",
      "[0.6609576]\n",
      "[0.40918982]\n",
      "[0.47960553]\n",
      "[0.40918982]\n",
      "[0.36061096]\n",
      "[0.9395917]\n",
      "[0.74416226]\n",
      "[0.33888882]\n",
      "[0.76449746]\n",
      "[0.47584227]\n",
      "[0.46955603]\n",
      "[1.4843423]\n",
      "[0.60647917]\n",
      "[1.0352356]\n",
      "[0.74164575]\n",
      "[0.53847194]\n",
      "[1.3535256]\n",
      "[0.33296075]\n",
      "[0.2328541]\n",
      "[1.0424498]\n",
      "[1.0352356]\n",
      "[0.46955603]\n",
      "[1.0352356]\n",
      "[1.0352356]\n",
      "[0.9896085]\n",
      "[0.66515857]\n",
      "[1.4257505]\n",
      "[0.8590058]\n",
      "[1.1644592]\n",
      "[0.29914585]\n",
      "[0.37633258]\n",
      "[0.6081469]\n",
      "[0.82134706]\n",
      "[0.40258926]\n",
      "[0.6081469]\n",
      "[0.1971079]\n",
      "[0.37633258]\n",
      "[0.46602184]\n",
      "[0.2593655]\n",
      "[0.7289803]\n",
      "[0.26004392]\n",
      "[0.45250306]\n",
      "[0.49084875]\n",
      "[1.0219392]\n",
      "[0.43931776]\n",
      "[0.26004392]\n",
      "[0.6875156]\n",
      "[0.55777687]\n",
      "[0.438748]\n",
      "[0.54655796]\n",
      "[0.66966903]\n",
      "[1.1997279]\n",
      "[0.51989007]\n",
      "[0.88343585]\n",
      "[1.5888206]\n",
      "[1.1526973]\n",
      "[0.6130438]\n",
      "[1.6513413]\n",
      "[0.83270514]\n",
      "[0.9175442]\n",
      "[0.83444434]\n",
      "[0.5144983]\n",
      "[0.6970682]\n",
      "[1.1997279]\n",
      "[1.1997279]\n",
      "[1.1997279]\n",
      "[0.6130438]\n",
      "[2.148628]\n",
      "[0.625479]\n",
      "[0.7924996]\n",
      "[0.35215393]\n",
      "[0.4098557]\n",
      "[0.8332623]\n",
      "[0.29655805]\n",
      "[0.6349143]\n",
      "[0.8154684]\n",
      "[0.6349143]\n",
      "[0.8332623]\n",
      "[1.4710008]\n",
      "[1.8613378]\n",
      "[0.9591279]\n",
      "[0.35215393]\n",
      "[0.5923276]\n",
      "[0.9343621]\n",
      "[1.4979562]\n",
      "[0.46763346]\n",
      "[1.0787206]\n",
      "[1.1593198]\n",
      "[0.29655805]\n",
      "[1.856458]\n",
      "[0.24782744]\n",
      "[0.44868934]\n",
      "[0.956914]\n",
      "[0.7404295]\n",
      "[0.5880364]\n",
      "[0.57457185]\n",
      "[1.1815673]\n",
      "[0.35401648]\n",
      "[1.1315546]\n",
      "[1.3169326]\n",
      "[0.6096356]\n",
      "[1.1315546]\n",
      "[1.0253744]\n",
      "[0.93656164]\n",
      "[0.71589905]\n",
      "[0.24782744]\n",
      "[0.7201005]\n",
      "[0.4059082]\n",
      "[0.58008045]\n",
      "[0.5078316]\n",
      "[0.4680653]\n",
      "[0.6417716]\n",
      "[0.22398093]\n",
      "[0.41333482]\n",
      "[0.5589893]\n",
      "[0.47205928]\n",
      "[0.8466916]\n",
      "[0.7536408]\n",
      "[0.2875591]\n",
      "[0.2195611]\n",
      "[1.0029997]\n",
      "[1.2799605]\n",
      "[0.8123692]\n",
      "[0.51246744]\n",
      "[0.8742518]\n",
      "[0.47808972]\n",
      "[0.41333482]\n",
      "[0.9125453]\n",
      "[0.7536408]\n",
      "[0.25429884]\n",
      "[0.3683402]\n",
      "[1.0137305]\n",
      "[0.36302188]\n",
      "[0.9866112]\n",
      "[0.62188977]\n",
      "[1.3158364]\n",
      "[0.76468146]\n",
      "[0.68658704]\n",
      "[0.3683402]\n",
      "[0.37983328]\n",
      "[1.0137305]\n",
      "[0.544151]\n",
      "[0.62188977]\n",
      "[0.42909914]\n",
      "[0.415464]\n",
      "[0.26751578]\n",
      "[1.1473298]\n",
      "[0.3683402]\n",
      "[0.89894074]\n",
      "[0.3184248]\n",
      "[0.63260484]\n",
      "[0.63260484]\n",
      "[0.18357717]\n",
      "[0.3079444]\n",
      "[0.5352851]\n",
      "[0.44775084]\n",
      "[0.5352851]\n",
      "[0.23893733]\n",
      "[0.21743618]\n",
      "[0.45269397]\n",
      "[0.43494496]\n",
      "[0.33139256]\n",
      "[0.21743618]\n",
      "[0.5352851]\n",
      "[0.3818692]\n",
      "[0.25431445]\n",
      "[0.18357717]\n",
      "[0.20415528]\n",
      "[0.63260484]\n",
      "[0.8169884]\n",
      "[0.20536725]\n",
      "[0.4268189]\n",
      "[0.4979896]\n",
      "[0.7979603]\n",
      "[0.3893309]\n",
      "[0.4325572]\n",
      "[0.4325572]\n",
      "[0.28564954]\n",
      "[0.32165]\n",
      "[0.8398523]\n",
      "[0.53417164]\n",
      "[0.28564954]\n",
      "[0.33906445]\n",
      "[0.47204572]\n",
      "[0.3927278]\n",
      "[0.20688263]\n",
      "[0.6673567]\n",
      "[0.303354]\n",
      "[0.71669024]\n",
      "[0.6515002]\n",
      "[0.84634084]\n",
      "[0.3612962]\n",
      "[0.27631238]\n",
      "[0.36805785]\n",
      "[0.31193224]\n",
      "[0.39873922]\n",
      "[0.42811638]\n",
      "[0.24919231]\n",
      "[0.3165132]\n",
      "[0.24919231]\n",
      "[0.36319068]\n",
      "[0.3626437]\n",
      "[0.32794416]\n",
      "[0.74676806]\n",
      "[0.44297254]\n",
      "[0.5355123]\n",
      "[0.6538597]\n",
      "[0.32261753]\n",
      "[0.5899765]\n",
      "[0.6540296]\n",
      "[0.63221216]\n",
      "[0.91641533]\n",
      "[0.9702852]\n",
      "[0.5615324]\n",
      "[0.30414504]\n",
      "[0.7838476]\n",
      "[0.63221216]\n",
      "[0.3002425]\n",
      "[0.276582]\n",
      "[0.7838476]\n",
      "[0.3512299]\n",
      "[0.46163198]\n",
      "[0.51678324]\n",
      "[0.4471573]\n",
      "[0.63221216]\n",
      "[0.276582]\n",
      "[0.8030217]\n",
      "[0.75323355]\n",
      "[0.7838476]\n",
      "[0.4471573]\n",
      "[0.3422225]\n",
      "[0.7310877]\n",
      "[0.7620334]\n",
      "[0.5571121]\n",
      "[0.5633884]\n",
      "[0.82636005]\n",
      "[0.55046093]\n",
      "[0.2845951]\n",
      "[0.43041158]\n",
      "[0.7925388]\n",
      "[0.55046093]\n",
      "[0.3577288]\n",
      "[0.31414008]\n",
      "[0.56733227]\n",
      "[0.5571121]\n",
      "[0.303448]\n",
      "[0.7310877]\n",
      "[0.29950216]\n",
      "[0.8850107]\n",
      "[0.8850107]\n",
      "[2.2049658]\n",
      "[1.0698729]\n",
      "[1.3342205]\n",
      "[0.3714786]\n",
      "[0.20644477]\n",
      "[1.8690916]\n",
      "[0.4105144]\n",
      "[0.5961351]\n",
      "[0.71281606]\n",
      "[2.230274]\n",
      "[2.605534]\n",
      "[0.94209397]\n",
      "[0.6260455]\n",
      "[2.0002613]\n",
      "[0.77525675]\n",
      "[0.7872194]\n",
      "[0.7413471]\n",
      "[2.2851896]\n",
      "[1.6297431]\n",
      "[0.7464151]\n",
      "[3.068912]\n",
      "[0.5048658]\n",
      "[0.530979]\n",
      "[1.3286424]\n",
      "[0.8400873]\n",
      "[2.871891]\n",
      "[3.1475813]\n",
      "[0.8807983]\n",
      "[2.8555381]\n",
      "[3.259601]\n",
      "[3.4259436]\n",
      "[1.2167512]\n",
      "[1.520441]\n",
      "[3.0584922]\n",
      "[0.8494133]\n",
      "[2.8992848]\n",
      "[0.8672624]\n",
      "[1.8588784]\n",
      "[2.5546904]\n",
      "[1.0490344]\n",
      "[1.7940441]\n",
      "[0.9122915]\n",
      "[0.6605126]\n",
      "[1.7425877]\n",
      "[2.0058103]\n",
      "[0.93416286]\n",
      "[1.9788916]\n",
      "[2.241488]\n",
      "[1.9727331]\n",
      "[1.1429636]\n",
      "[2.064887]\n",
      "[1.0350021]\n",
      "[2.0897589]\n",
      "[1.5962995]\n",
      "[0.49975863]\n",
      "[1.1530266]\n",
      "[0.39235908]\n",
      "[2.167542]\n",
      "[1.9928305]\n",
      "[0.35114864]\n",
      "[0.43427962]\n",
      "[1.6453911]\n",
      "[0.6026639]\n",
      "[0.19979298]\n",
      "[0.43144]\n",
      "[2.3462949]\n",
      "[2.0149708]\n",
      "[0.4630417]\n",
      "[1.266948]\n",
      "[1.7513112]\n",
      "[1.1177577]\n",
      "[0.46414617]\n",
      "[0.57382953]\n",
      "[0.47449645]\n",
      "[2.0957289]\n",
      "[1.6323313]\n",
      "[0.97110337]\n",
      "[1.9365203]\n",
      "[0.5121341]\n",
      "[1.5540031]\n",
      "[1.9418322]\n",
      "[1.8273054]\n",
      "[0.41919252]\n",
      "[1.2095672]\n",
      "[0.6071158]\n",
      "[0.4926605]\n",
      "[2.7168922]\n",
      "[2.8486354]\n",
      "[1.9952492]\n",
      "[0.5532166]\n",
      "[2.78524]\n",
      "[2.509405]\n",
      "[1.3776858]\n",
      "[0.9880696]\n",
      "[2.4379237]\n",
      "[2.1974316]\n",
      "[2.5450299]\n",
      "[0.73881567]\n",
      "[0.5011886]\n",
      "[0.64538497]\n",
      "[1.8591052]\n",
      "[1.0917819]\n",
      "[1.9667728]\n",
      "[0.7668338]\n",
      "[0.48524463]\n",
      "[2.7163339]\n",
      "[0.80659014]\n",
      "[0.36564618]\n",
      "[1.6350443]\n",
      "[1.7416823]\n",
      "[1.8964491]\n",
      "[2.1631575]\n",
      "[0.28984755]\n",
      "[0.38163054]\n",
      "[0.49451986]\n",
      "[1.6241847]\n",
      "[2.5855181]\n",
      "[0.388021]\n",
      "[1.6946863]\n",
      "[1.224575]\n",
      "[2.5371587]\n",
      "[2.1681957]\n",
      "[2.217334]\n",
      "[0.7837959]\n",
      "[1.8467562]\n",
      "[2.1840599]\n",
      "[1.4502004]\n",
      "[0.6949433]\n",
      "[0.57709265]\n",
      "[0.361253]\n",
      "[0.52370304]\n",
      "[2.181526]\n",
      "[0.73286223]\n",
      "[2.5246255]\n",
      "[2.494411]\n",
      "[2.3104475]\n",
      "[0.8240079]\n",
      "[2.5693476]\n",
      "[1.7976056]\n",
      "[2.4071078]\n",
      "[2.6092067]\n",
      "[0.59432656]\n",
      "[2.8552873]\n",
      "[2.4170935]\n",
      "[0.80769175]\n",
      "[0.1781556]\n",
      "[2.2104008]\n",
      "[0.80727965]\n",
      "[0.83196944]\n",
      "[2.787663]\n",
      "[0.5892907]\n",
      "[0.5828624]\n",
      "[0.7524117]\n",
      "[2.3260508]\n",
      "[2.5357256]\n",
      "[1.2704476]\n",
      "[0.4006272]\n",
      "[2.05463]\n",
      "[2.6841288]\n",
      "[2.6841288]\n",
      "[2.472134]\n",
      "[2.313802]\n",
      "[0.499462]\n",
      "[2.0968206]\n",
      "[0.32685968]\n",
      "[1.2711868]\n",
      "[0.6713878]\n",
      "[0.44635388]\n",
      "[0.24125074]\n",
      "[0.87632394]\n",
      "[0.66764927]\n",
      "[1.0677656]\n",
      "[2.331243]\n",
      "[1.2482067]\n",
      "[0.7272827]\n",
      "[0.9663877]\n",
      "[1.9782516]\n",
      "[2.427331]\n",
      "[2.765977]\n",
      "[2.5254]\n",
      "[0.95437014]\n",
      "[2.330162]\n",
      "[2.3120265]\n",
      "[2.275237]\n",
      "[1.7221524]\n",
      "[1.1689532]\n",
      "[1.5647875]\n",
      "[0.86523795]\n",
      "[2.5569737]\n",
      "[0.8092625]\n",
      "[2.6949492]\n",
      "[2.3522854]\n",
      "[1.7251631]\n",
      "[3.0635543]\n",
      "[0.46224284]\n",
      "[3.0635543]\n",
      "[1.8270446]\n",
      "[3.2656364]\n",
      "[1.0362062]\n",
      "[0.9828374]\n",
      "[0.297282]\n",
      "[0.66703916]\n",
      "[1.9238385]\n",
      "[1.1805601]\n",
      "[1.2783247]\n",
      "[1.7722585]\n",
      "[2.055757]\n",
      "[0.37681666]\n",
      "[0.7133113]\n",
      "[1.6919031]\n",
      "[2.1195261]\n",
      "[1.7355117]\n",
      "[0.8137525]\n",
      "[0.7424691]\n",
      "[1.6766878]\n",
      "[2.4274101]\n",
      "[1.0643109]\n",
      "[2.2353215]\n",
      "[1.3907611]\n",
      "[0.5155076]\n",
      "[2.2147348]\n",
      "[2.0047696]\n",
      "[2.5740242]\n",
      "[1.5887018]\n",
      "[0.31597778]\n",
      "[0.795586]\n",
      "[1.3703685]\n",
      "[0.21547547]\n",
      "[2.1570587]\n",
      "[0.19656035]\n",
      "[2.7597725]\n",
      "[1.826137]\n",
      "[0.23830476]\n",
      "[0.23830476]\n",
      "[0.7280386]\n",
      "[0.5691994]\n",
      "[1.328861]\n",
      "[1.3352737]\n",
      "[2.291241]\n",
      "[0.46698588]\n",
      "[0.97057533]\n",
      "[0.26664498]\n",
      "[0.92554724]\n",
      "[2.1859863]\n",
      "[0.49221066]\n",
      "[1.2286799]\n",
      "[2.968736]\n",
      "[0.47545108]\n",
      "[1.134676]\n",
      "[1.5252694]\n",
      "[1.4219785]\n",
      "[2.4820013]\n",
      "[2.3462915]\n",
      "[1.9939103]\n",
      "[2.4252026]\n",
      "[2.1823974]\n",
      "[2.683358]\n",
      "[0.6331051]\n",
      "[0.64928854]\n",
      "[2.2207043]\n",
      "[1.1416447]\n",
      "[0.599842]\n",
      "[1.518684]\n",
      "[0.4297659]\n",
      "[1.7471348]\n",
      "[0.505439]\n",
      "[1.0007671]\n",
      "[1.0074859]\n",
      "[1.7474269]\n",
      "[1.6700652]\n",
      "[2.3033533]\n",
      "[1.201058]\n",
      "[1.4212571]\n",
      "[1.1807028]\n",
      "[1.1757793]\n",
      "[2.056654]\n",
      "[1.036839]\n",
      "[1.460313]\n",
      "[0.20555367]\n",
      "[1.3675749]\n",
      "[0.48178643]\n",
      "[0.20584197]\n",
      "[0.6689827]\n",
      "[0.6485623]\n",
      "[2.521782]\n",
      "[1.0205469]\n",
      "[0.7163201]\n",
      "[3.2131119]\n",
      "[1.7963609]\n",
      "[0.5944952]\n",
      "[0.75978035]\n",
      "[1.7142057]\n",
      "[0.9546438]\n",
      "[0.3898777]\n",
      "[0.387152]\n",
      "[1.8669683]\n",
      "[2.4045622]\n",
      "[0.3688897]\n",
      "[1.8347728]\n",
      "[0.62280536]\n",
      "[2.1620848]\n",
      "[0.61846983]\n",
      "[1.0141021]\n",
      "[1.0427715]\n",
      "[1.7976056]\n",
      "[0.9285254]\n",
      "[2.007529]\n",
      "[2.1444387]\n",
      "[2.1054478]\n",
      "[0.56711864]\n",
      "[1.7320107]\n",
      "[1.7928087]\n",
      "[2.0337672]\n",
      "[2.110152]\n",
      "[0.5515685]\n",
      "[2.7806652]\n",
      "[0.4019594]\n",
      "[0.15017822]\n",
      "[0.44381616]\n",
      "[1.8361267]\n",
      "[3.2108579]\n",
      "[1.734368]\n",
      "[2.2245107]\n",
      "[2.0646434]\n",
      "[0.52148205]\n",
      "[1.4398174]\n",
      "[0.64880997]\n",
      "[1.4576892]\n",
      "[0.43810633]\n",
      "[2.0713112]\n",
      "[2.4525557]\n",
      "[2.619793]\n",
      "[1.9594834]\n",
      "[0.6555141]\n",
      "[2.6454582]\n",
      "[2.2716043]\n",
      "[2.490676]\n",
      "[2.6891763]\n",
      "[0.75497377]\n",
      "[0.6041015]\n",
      "[3.130043]\n",
      "[2.9853437]\n",
      "[2.476574]\n",
      "[1.9788916]\n",
      "[2.1724832]\n",
      "[2.4278631]\n",
      "[2.9135592]\n",
      "[2.4117148]\n",
      "[0.6583601]\n",
      "[2.3285327]\n",
      "[1.7827927]\n",
      "[0.5747428]\n",
      "[3.0457492]\n",
      "[0.55101657]\n",
      "[0.56778556]\n",
      "[2.4310648]\n",
      "[1.4177909]\n",
      "[2.6649494]\n",
      "[2.251979]\n",
      "[0.73373497]\n",
      "[2.1842775]\n",
      "[2.1225364]\n",
      "[2.5697346]\n",
      "[2.0368786]\n",
      "[0.3279083]\n",
      "[0.92440593]\n",
      "[1.2259666]\n",
      "[0.573719]\n",
      "[2.1300366]\n",
      "[1.2368418]\n",
      "[2.129084]\n",
      "[0.32116348]\n",
      "[0.6444568]\n",
      "[0.24623545]\n",
      "[0.5381419]\n",
      "[2.2624846]\n",
      "[0.4874638]\n",
      "[2.0651193]\n",
      "[1.4849265]\n",
      "[1.3462754]\n",
      "[0.7809706]\n",
      "[2.0594852]\n",
      "[2.0625844]\n",
      "[0.66016746]\n",
      "[1.7562895]\n",
      "[2.0365741]\n",
      "[2.4577374]\n",
      "[0.32536012]\n",
      "[1.8147639]\n",
      "[2.1518295]\n",
      "[2.178698]\n",
      "[2.5515227]\n",
      "[1.8733873]\n",
      "[2.3160584]\n",
      "[2.7638333]\n",
      "[2.8926396]\n",
      "[0.3490444]\n",
      "[2.715982]\n",
      "[2.275938]\n",
      "[0.29103366]\n"
     ]
    }
   ],
   "source": [
    "len(predictions)\n",
    "count = 0\n",
    "for x,y in zip(predictions,test_labels):\n",
    "    temp = 0\n",
    "    print(x)\n",
    "    if x < 0.5:\n",
    "        temp=1\n",
    "    if temp==y:\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
